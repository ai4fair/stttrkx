import sys
import os
import copy
import logging
import tracemalloc
import gc
from memory_profiler import profile

from pytorch_lightning.callbacks import Callback
import torch.nn.functional as F
import sklearn.metrics
import matplotlib.pyplot as plt
import torch
import numpy as np
from sklearn.metrics import roc_curve

"""Class-based Callback Inference for Integration with Pytorch Lightning"""

"""
Explanation of Evalution Metrics:

1.     ROC: FPR vs TPR
2.     PRC: PPV vs TPR
3. EFF-PUR: TNR vs TPR


# Alternate Names

- TPR is also known as Sensitivity, Recall, Hit Rate or Probability of Detection. To calculate: TPR = 1 - FNR
- TNR is alos known as Specificity, Selectivity. To calculate: TNR = 1 - FPR
- FPR is also known as Fall-out or Probability of False Alarm. To calculate: FPR = 1 - TNR = 1 âˆ’ specificity
- FNR is also known as Miss rate. To calculate: FNR = 1 - TPR

- Accuracy. To calculate: ACC = (TP + TN)/(TP + TN + FP + FN)
- Precision is also known as Positive Predictive Value (PPV). To calculate: PPV = 1 - FDR = TP/(TP + FP)
- F1-score is harmonic mean of precision and recall

# HEP Metric
- Efficiency := TPR or Sensitivity or Recall or Probability of Detection
-     Purity := TNR or Specificity or Selectivity

From ROC curve, one can exatract these variables. For example, the ROC curve gives

`roc_fpr, roc_tpr, roc_thr = sklearn.metrics.roc_curve(truth, preds)`

Efficiency := roc_tpr
    Purity := 1 - roc_fpr

So, the efficiency vs purity curve is infact TPR vs TNR curve.
"""


# GNNMetrics Callback
class GNNMetrics(Callback):

    """Simpler version of 'GNNTelemetry' callback. It contains standardised
    tests (AUC-ROC & AUC-PRC curves) of the performance of a GNN network."""

    def __init__(self):
        super().__init__()
        logging.info("Constructing GNNMetrics Callback !")

    def on_test_start(self, trainer, pl_module):

        """This hook is automatically called when the model is tested
        after training. The best checkpoint is automatically loaded"""
        self.preds = []
        self.truth = []
        
        print("Starting GNNMetrics...")

    def on_test_batch_end(
        self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx
    ):

        """Get the relevant outputs from each batch"""

        self.preds.append(outputs["preds"])
        self.truth.append(outputs["truth"])

    def on_test_end(self, trainer, pl_module):

        """ 
         1. Aggregate all outputs,
         2. Calculate the ROC/PRC curve,
         3. Plot ROC/PRC curve,
         4. Save plots to PDF 'AUC-ROC/PRC.pdf'
        """

        # TODO: REFACTOR THIS INTO CALCULATE METRICS, PLOT METRICS, SAVE METRICS
        
        # Aggregate 'truth' and 'pred' from all batches.
        preds = np.concatenate(self.preds)
        truth = np.concatenate(self.truth)

        print("Starting GNNMetrics...\n")
        print(preds.shape, truth.shape)


        # -------------------------- ROC Metric
        # AUC-ROC:: Calculate 
        roc_fpr, roc_tpr, roc_thr = sklearn.metrics.roc_curve(truth, preds)
        roc_auc = sklearn.metrics.auc(roc_fpr, roc_tpr)
        logging.info("ROC AUC: %s", roc_auc)
        
        
        # AUC-ROC:: Plotting
        fig, axs = plt.subplots(nrows=1, ncols=1, figsize=(10,10))
        axs = axs.flatten() if type(axs) is list else [axs]
        
        axs[0].plot(roc_fpr, roc_tpr, color="darkorange", label="ROC Curve, AUC = %.3f" % roc_auc)
        axs[0].plot([0, 1], [0, 1], color="navy", linestyle="--")
        axs[0].set_xlabel("False Positive Rate", fontsize=20)
        axs[0].set_ylabel("True Positive Rate", fontsize=20)
        axs[0].set_title("ROC Curve, AUC = %.3f" % roc_auc)
        axs[0].legend(loc='lower right')
        plt.tight_layout()
        fig.savefig("roc_curve.pdf", format="pdf")


        # -------------------------- PRC Metric
        # AUC-PRC: Calculate
        # ppv, tpr, thr = sklearn.metrics.precision_recall_curve(truth, preds)
        pre, recall, thr = sklearn.metrics.precision_recall_curve(truth, preds)
        prc_auc = sklearn.metrics.auc(recall, pre)
        logging.info("PRC AUC: %s", prc_auc)
        
        
        # AUC-PRC:: Plotting
        fig, axs = plt.subplots(nrows=1, ncols=1, figsize=(10,10))
        axs = axs.flatten() if type(axs) is list else [axs]

        axs[0].plot(recall, pre, color="darkorange", label="ROC Curve, AUC = %.3f" % prc_auc)
        axs[0].plot([0, 1], [1, 0], color="navy", linestyle="--")
        axs[0].set_xlabel("Recall/TPR", fontsize=20)
        axs[0].set_ylabel("Precision/PPV", fontsize=20)
        axs[0].set_title("PRC Curve, AUC = %.3f" % prc_auc)
        axs[0].legend(loc='lower left')
        plt.tight_layout()
        fig.savefig("prc_curve.pdf", format="pdf")


        # -------------------------- Eff-Pur Metric
        # Efficiency-Purity: Calculate
        eff = roc_tpr
        pur = 1 - roc_fpr
        eff_pur_auc = sklearn.metrics.auc(eff, pur)
        logging.info("EFF-PUR AUC: %s", eff_pur_auc)
        
        
        # EFF-PUR:: Plotting
        fig, axs = plt.subplots(nrows=1, ncols=1, figsize=(10,10))
        axs = axs.flatten() if type(axs) is list else [axs]

        axs[0].plot(eff, pur, color="darkorange", label="ROC Curve, AUC = %.5f" % eff_pur_auc)
        axs[0].plot([0, 1], [1, 0], color="navy", linestyle="--")
        axs[0].set_xlabel("Efficiency", fontsize=20)
        axs[0].set_ylabel("Purity", fontsize=20)
        axs[0].set_title("Efficiency-Purity Curve, AUC = %.3f" % eff_pur_auc)
        axs[0].legend(loc='lower left')
        plt.tight_layout()
        fig.savefig("eff_pur_curve.png", format="png")


    def make_plot(self, x_val, y_val, x_lab, y_lab, title, legend):
        """Common function for creating plots"""
        
        # Update this to dynamically adapt to number of metrics
        fig, axs = plt.subplots(nrows=1, ncols=1, figsize=(10,10))
        axs = axs.flatten() if type(axs) is list else [axs]

        axs[0].plot(x_val, y_val, color="darkorange", label=title)
        axs[0].set_xlabel(x_lab, fontsize=20)
        axs[0].set_ylabel(y_lab, fontsize=20)
        axs[0].set_title(title)
        axs[0].legend()
        plt.tight_layout()

        return fig, axs


# GNNTelemetry Callback
class GNNTelemetry(Callback):

    """
    This callback contains standardised tests of the performance of a GNN
    """

    def __init__(self):
        super().__init__()
        logging.info("Constructing telemetry callback")

    def on_test_start(self, trainer, pl_module):

        """
        This hook is automatically called when the model is tested after training. The best checkpoint is automatically loaded
        """
        self.preds = []
        self.truth = []

        print("Starting TELEMETRY")

    def on_test_batch_end(
        self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx
    ):

        """Get the relevant outputs from each batch"""

        self.preds.append(outputs["preds"].cpu())
        self.truth.append(outputs["truth"].cpu())


    def on_test_end(self, trainer, pl_module):

        """
        1. Aggregate all outputs,
        2. Calculate the ROC curve,
        3. Plot ROC curve,
        4. Save plots to PDF 'metrics.pdf'
        """
        
        metrics = self.calculate_metrics()
        metrics_plots = self.plot_metrics(metrics)
        self.save_metrics(metrics_plots, pl_module.hparams.output_dir)


    def calculate_metrics(self):
        """"Calculate metrics"""
        eff, pur, score_cuts, auc = self.get_eff_pur_metrics()

        return {
            "eff_plot": {"eff": eff, "score_cuts": score_cuts},
            "pur_plot": {"pur": pur, "score_cuts": score_cuts},
            "auc_plot": {"eff": eff, "pur": pur, "auc": auc},
        }


    def get_eff_pur_metrics(self):

        self.truth = torch.cat(self.truth)
        self.preds = torch.cat(self.preds)
        
        # AUC-PRC Curve
        # precision, recall, thresholds = precision_recall_curve(y_true, y_scores)

        # AUC-ROC Curve
        # fpr, tpr, thr = roc_curve(y_true, y_score)
        fpr, eff, score_cuts = sklearn.metrics.roc_curve(self.truth, self.preds)
        pur = 1 - fpr

        # Area Under the Curve (AUC) using the trapezoidal rule.
        # auc = sklearn.metrics.auc(x, y)
        auc = sklearn.metrics.auc(eff, pur)

        eff, pur, score_cuts = (
            eff[score_cuts <= 1],
            pur[score_cuts <= 1],
            score_cuts[score_cuts <= 1],
        )  # Make sure this is nicely plottable!

        return eff, pur, score_cuts, auc



    def make_plot(self, x_val, y_val, x_lab, y_lab, title):

        # Update this to dynamically adapt to number of metrics
        fig, axs = plt.subplots(nrows=1, ncols=1, figsize=(10,10))
        axs = axs.flatten() if type(axs) is list else [axs]

        axs[0].plot(x_val, y_val)
        axs[0].set_xlabel(x_lab, fontsize=20)
        axs[0].set_ylabel(y_lab, fontsize=20)
        axs[0].set_title(title)
        plt.tight_layout()

        return fig, axs

    def plot_metrics(self, metrics):
        
        # Efficiency vs Threshold (cuts)
        eff_fig, eff_axs = self.make_plot(
            metrics["eff_plot"]["score_cuts"],
            metrics["eff_plot"]["eff"],
            "Cut",
            "Efficiency",
            "Efficiency vs. Cut",
        )
        
        # Purity vs Threshold (cuts)
        pur_fig, pur_axs = self.make_plot(
            metrics["pur_plot"]["score_cuts"],
            metrics["pur_plot"]["pur"],
            "Cut",
            "Purity",
            "Purity vs. Cut",
        )
        
        # Purity vs Efficiency
        auc_fig, auc_axs = self.make_plot(
            metrics["auc_plot"]["eff"],
            metrics["auc_plot"]["pur"],
            "Efficiency",
            "Purity",
            "Efficiency-Purity Curve, AUC = %.3f" % metrics["auc_plot"]["auc"],
        )
        
        # TODO: How to add AUC?
        #auc_axs.text(0.8, 0.8, 'boxed italics text in data coords', style='italic',
        #bbox={'facecolor': 'red', 'alpha': 0.5, 'pad': 10})
        
        return {
            "eff_plot": [eff_fig, eff_axs],
            "pur_plot": [pur_fig, pur_axs],
            "auc_plot": [auc_fig, auc_axs],
        }

    def save_metrics(self, metrics_plots, output_dir):

        os.makedirs(output_dir, exist_ok=True)

        for metric, (fig, axs) in metrics_plots.items():
            fig.savefig(os.path.join(output_dir, f"metrics_{metric}.png"), format="png")




#FIXME::ADAK To get the output files as integers change batch.event_file[-4:] to str(int(batch.event_file[-4:])). 
# Note that the one needs string type for torch.save(), so from 'str' to 'int' followed by 'str'. The event_file
# is of the format e.g. path/to/event0000000001, so event_file[-10:] will return 0000000001 (last 10 str) and 
# event_file[-4:] will return 0001 (last 4 str). We are not expecting more than 9999 event in testset. The trainset
# and valset are rebuild with GNNBuilder but not needed as both are redundant w.r.t data from Processing.

# GNNBuilder Callback
class GNNBuilder(Callback):
    """Callback handling filter inference for later stages.

    This callback is used to apply a trained filter model to the dataset of a LightningModule.
    The data structure is preloaded in the model, as training, validation and testing sets.
    Intended usage: run training and examine the telemetry to decide on the hyperparameters (e.g. r_test) that
    lead to desired efficiency-purity tradeoff. Then set these hyperparameters in the pipeline configuration and run
    with the --inference flag. Otherwise, to just run straight through automatically, train with this callback included.

    """

    def __init__(self):
        self.output_dir = None
        self.overwrite = False

    def on_test_end(self, trainer, pl_module):

        print("Testing finished, running inference to build graphs...")

        datasets = self.prepare_datastructure(pl_module)

        total_length = sum([len(dataset) for dataset in datasets.values()])

        pl_module.eval()
        with torch.no_grad():
            batch_incr = 0
            for set_idx, (datatype, dataset) in enumerate(datasets.items()):
                for batch_idx, batch in enumerate(dataset):
                    percent = (batch_incr / total_length) * 100
                    sys.stdout.flush()
                    sys.stdout.write(f"{percent:.01f}% inference complete \r")
                    if (
                        not os.path.exists(
                            os.path.join(
                                self.output_dir, datatype, str(int(batch.event_file[-4:])) #ADAK [-4:] to str(int([-4:]))
                            )
                        )
                    ) or self.overwrite:
                        batch_to_save = copy.deepcopy(batch)
                        batch_to_save = batch_to_save.to(
                            pl_module.device
                        )  # Is this step necessary??
                        self.construct_downstream(batch_to_save, pl_module, datatype)

                    batch_incr += 1

    def prepare_datastructure(self, pl_module):
        # Prep the directory to produce inference data to
        self.output_dir = pl_module.hparams.output_dir
        self.datatypes = ["train", "val", "test"]

        os.makedirs(self.output_dir, exist_ok=True)
        [
            os.makedirs(os.path.join(self.output_dir, datatype), exist_ok=True)
            for datatype in self.datatypes
        ]

        # Set overwrite setting if it is in config
        self.overwrite = (
            pl_module.hparams.overwrite if "overwrite" in pl_module.hparams else False
        )

        # By default, the set of examples propagated through the pipeline will be train+val+test set
        datasets = {
            "train": pl_module.trainset,
            "val": pl_module.valset,
            "test": pl_module.testset,
        }

        return datasets

    def construct_downstream(self, batch, pl_module, datatype):

        output = pl_module(
            pl_module.get_input_data(batch),
            torch.cat([batch.edge_index, batch.edge_index.flip(0)], dim=-1),
        ).squeeze()
        batch.scores = torch.sigmoid(output)

        self.save_downstream(batch, pl_module, datatype)

    def save_downstream(self, batch, pl_module, datatype):
        
        with open(
            os.path.join(self.output_dir, datatype, str(int(batch.event_file[-4:]))), "wb" #ADAK [-4:] to str(int([-4:]))
        ) as pickle_file:
            torch.save(batch, pickle_file)

        logging.info("Saved event {}".format(batch.event_file[-4:]))
